{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPunmp9lQviZ/yqemEETdZh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanush852/intro_to-ml/blob/main/HOME_WORK_7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdACc51iuJ3N",
        "outputId": "4d0c9e76-9dd5-409b-d5e0-5a6ee4a4987e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.12)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1\n",
            "time: 413 µs (started: 2023-12-13 01:09:53 +00:00)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)\n",
        "np.random.seed(123)\n"
      ],
      "metadata": {
        "id": "ne1rl57yuN_k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
        "device\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDszB7YJuckF",
        "outputId": "0ebae3f1-c068-4510-df8f-1902d7335291"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w2anKoB0ujk1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "imgs = torch.stack([img for img, _ in train_dataset], dim=3)\n",
        "mean, std = imgs.view(3, -1).mean(dim=1), imgs.view(3, -1).std(dim=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j30jrzorvYeY",
        "outputId": "d5e7b979-433a-4c96-a6a1-27620feb8a75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 47625515.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlbiw7Pvvw9u",
        "outputId": "42652ab3-7bd3-45c7-c752-9d64a8f4b8d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.4914, 0.4822, 0.4465]), tensor([0.2470, 0.2435, 0.2616]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cifar10 = datasets.CIFAR10('./data', train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "id": "jzRQqKxgv4Nt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hf59UnjkwlFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "     './data', train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "id": "aKJ72BUwwfrU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoad = DataLoader(cifar10, batch_size=32, shuffle=True, num_workers=2)\n",
        "testLoad = DataLoader(cifar10_val, batch_size=32, shuffle=False, num_workers=2)\n",
        "classNames = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "metadata": {
        "id": "DGbi-oSswmA3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "O6LmOcrTwsUr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)  # <1>\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 2 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "-PEEXJbhwvvf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, n_chans):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
        "                              padding=1, bias=False)  # <1>\n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
        "                                      nonlinearity='relu')  # <2>\n",
        "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.batch_norm(out)\n",
        "        out = torch.relu(out)\n",
        "        return out + x"
      ],
      "metadata": {
        "id": "0VKOqO0rxEAh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "1atk_69OxJR9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetResDeep(nn.Module):\n",
        "    def __init__(self, n_chans1=32, n_blocks=10):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = self.resblocks(out)\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "gnGvgOn2xNKL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = trainLoad,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfA6cQzgxj2D",
        "outputId": "a281e6b5-20dd-4787-ccd2-9230fe97267f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 01:27:03.845440 Epoch 1, Training loss 1.6408428361762126\n",
            "2023-12-13 01:27:35.494973 Epoch 2, Training loss 1.3125485861751411\n",
            "2023-12-13 01:28:38.287586 Epoch 4, Training loss 1.0913012855646325\n",
            "2023-12-13 01:29:42.265201 Epoch 6, Training loss 0.9693882665951437\n",
            "2023-12-13 01:30:45.141597 Epoch 8, Training loss 0.8891217424102266\n",
            "2023-12-13 01:31:49.668626 Epoch 10, Training loss 0.8263548747782362\n",
            "2023-12-13 01:32:52.954891 Epoch 12, Training loss 0.7709218527900051\n",
            "2023-12-13 01:33:57.595598 Epoch 14, Training loss 0.7286793912212131\n",
            "2023-12-13 01:35:00.809796 Epoch 16, Training loss 0.6925601780757794\n",
            "2023-12-13 01:36:05.758714 Epoch 18, Training loss 0.6614179966240759\n",
            "2023-12-13 01:37:08.994209 Epoch 20, Training loss 0.6252877299345538\n",
            "2023-12-13 01:38:12.631133 Epoch 22, Training loss 0.5959548180874006\n",
            "2023-12-13 01:39:16.895220 Epoch 24, Training loss 0.5688352908221713\n",
            "2023-12-13 01:40:20.730564 Epoch 26, Training loss 0.5410055098477191\n",
            "2023-12-13 01:41:23.297320 Epoch 28, Training loss 0.5157140052316667\n",
            "2023-12-13 01:42:27.196233 Epoch 30, Training loss 0.4912729534396207\n",
            "2023-12-13 01:43:29.581497 Epoch 32, Training loss 0.4694784354902351\n",
            "2023-12-13 01:44:32.544915 Epoch 34, Training loss 0.44904822647399956\n",
            "2023-12-13 01:45:35.493192 Epoch 36, Training loss 0.42509246553203195\n",
            "2023-12-13 01:46:37.825929 Epoch 38, Training loss 0.40678717327552655\n",
            "2023-12-13 01:47:41.243309 Epoch 40, Training loss 0.3844042146627291\n",
            "2023-12-13 01:48:43.210284 Epoch 42, Training loss 0.36632347089296263\n",
            "2023-12-13 01:49:46.561580 Epoch 44, Training loss 0.3469535485625038\n",
            "2023-12-13 01:50:48.661286 Epoch 46, Training loss 0.3330608437687483\n",
            "2023-12-13 01:51:52.050278 Epoch 48, Training loss 0.3152498519518821\n",
            "2023-12-13 01:52:53.894343 Epoch 50, Training loss 0.2994990941072723\n",
            "2023-12-13 01:53:56.764483 Epoch 52, Training loss 0.28749862982654945\n",
            "2023-12-13 01:54:59.596871 Epoch 54, Training loss 0.2681938960376033\n",
            "2023-12-13 01:56:01.715596 Epoch 56, Training loss 0.2567126665297736\n",
            "2023-12-13 01:57:04.635189 Epoch 58, Training loss 0.24080821414616957\n",
            "2023-12-13 01:58:06.884451 Epoch 60, Training loss 0.235431190971011\n",
            "2023-12-13 01:59:09.934015 Epoch 62, Training loss 0.22523020148773035\n",
            "2023-12-13 02:00:11.678660 Epoch 64, Training loss 0.2093865242694348\n",
            "2023-12-13 02:01:14.984158 Epoch 66, Training loss 0.1990763042348551\n",
            "2023-12-13 02:02:17.098907 Epoch 68, Training loss 0.19895426468400526\n",
            "2023-12-13 02:03:22.323175 Epoch 70, Training loss 0.18367925328896717\n",
            "2023-12-13 02:04:46.895021 Epoch 72, Training loss 0.17584116518037465\n",
            "2023-12-13 02:06:01.886462 Epoch 74, Training loss 0.17344098607637107\n",
            "2023-12-13 02:07:11.834017 Epoch 76, Training loss 0.15907363918438908\n",
            "2023-12-13 02:08:17.455912 Epoch 78, Training loss 0.15789513234394678\n",
            "2023-12-13 02:09:22.381488 Epoch 80, Training loss 0.151153983711846\n",
            "2023-12-13 02:10:37.581340 Epoch 82, Training loss 0.14404799120878456\n",
            "2023-12-13 02:11:41.667607 Epoch 84, Training loss 0.1343533144816899\n",
            "2023-12-13 02:12:46.244042 Epoch 86, Training loss 0.13194489622874733\n",
            "2023-12-13 02:13:50.684983 Epoch 88, Training loss 0.1275772620230837\n",
            "2023-12-13 02:14:53.269086 Epoch 90, Training loss 0.12567231832495196\n",
            "2023-12-13 02:15:56.745602 Epoch 92, Training loss 0.11819664590383665\n",
            "2023-12-13 02:16:59.491062 Epoch 94, Training loss 0.11394221321840532\n",
            "2023-12-13 02:18:04.236341 Epoch 96, Training loss 0.11335017263647396\n",
            "2023-12-13 02:19:12.824981 Epoch 98, Training loss 0.0986733738319878\n",
            "2023-12-13 02:20:24.581125 Epoch 100, Training loss 0.10306882760138922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "def validate(model, train_loader, val_loader):\n",
        "    accdict = {}\n",
        "    predictions = []\n",
        "    exp_labels = []\n",
        "\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "                predictions.extend(predicted.cpu().numpy())\n",
        "                exp_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "        accdict[name] = correct / total\n",
        "    return accdict, predictions, exp_labels\n"
      ],
      "metadata": {
        "id": "_PqvaUZK6YUM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, predictions, expected_labels = validate(model, train_loader, val_loader)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t4kV8j96fKo",
        "outputId": "cbdca36c-3b95-4dcb-d089-f520c9d5fbd4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.95\n",
            "Accuracy val: 0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_d = precision_score(predictions, expected_labels, average='macro')\n",
        "recall_d = recall_score(predictions, expected_labels, average='macro')\n",
        "cnfMatrix = confusion_matrix(predictions, expected_labels)\n",
        "print(cnfMatrix)"
      ],
      "metadata": {
        "id": "9xbIp5ry6nta"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions, expected_labels, target_names=classNames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__yfm4fD66C-",
        "outputId": "d9092433-b93b-43f2-9faa-9c3deabd0094"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.96      0.81      0.88      7098\n",
            "  automobile       0.93      0.94      0.94      5968\n",
            "        bird       0.85      0.92      0.88      5560\n",
            "         cat       0.89      0.82      0.85      6550\n",
            "        deer       0.89      0.91      0.90      5887\n",
            "         dog       0.83      0.94      0.88      5333\n",
            "        frog       0.93      0.94      0.94      5919\n",
            "       horse       0.95      0.88      0.92      6458\n",
            "        ship       0.96      0.91      0.94      6306\n",
            "       truck       0.80      0.98      0.88      4921\n",
            "\n",
            "    accuracy                           0.90     60000\n",
            "   macro avg       0.90      0.91      0.90     60000\n",
            "weighted avg       0.91      0.90      0.90     60000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetWidth(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1yDnvC737UDV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
        "                        train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            l2_lambda = 0.001\n",
        "            l2_norm = sum(p.pow(2.0).sum()\n",
        "                          for p in model.parameters())  # <1>\n",
        "            loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))\n"
      ],
      "metadata": {
        "id": "HaagUh4z7avK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "modelWd = NetWidth(n_chans1=32).to(device=device)\n",
        "optimizer_wd = optim.SGD(modelWd.parameters(), lr=1e-2)\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "training_loop_l2reg(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer_wd,\n",
        "    model = modelWd,\n",
        "    loss_fn = lossFn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO9TgY-98Bcn",
        "outputId": "c74fd699-0443-4480-f9f4-899b07c77e99"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 02:22:06.371203 Epoch 1, Training loss 2.0321958574187726\n",
            "2023-12-13 02:24:39.956756 Epoch 10, Training loss 1.146674206952\n",
            "2023-12-13 02:27:29.264053 Epoch 20, Training loss 0.9690431043162675\n",
            "2023-12-13 02:30:20.476496 Epoch 30, Training loss 0.8994943727465237\n",
            "2023-12-13 02:33:10.151513 Epoch 40, Training loss 0.8590997023808072\n",
            "2023-12-13 02:35:59.973098 Epoch 50, Training loss 0.831738175104951\n",
            "2023-12-13 02:38:49.145809 Epoch 60, Training loss 0.811712636819581\n",
            "2023-12-13 02:41:38.891280 Epoch 70, Training loss 0.7966082192138028\n",
            "2023-12-13 02:44:27.072983 Epoch 80, Training loss 0.7848985903250897\n",
            "2023-12-13 02:47:16.535805 Epoch 90, Training loss 0.7753838341864173\n",
            "2023-12-13 02:50:05.235128 Epoch 100, Training loss 0.7674891792447366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracyWd, predictionsWd, expected_labels_wd = validate(modelWd, train_loader, val_loader)\n",
        "precisionWd = precision_score(predictionsWd, expected_labels_wd, average='macro')\n",
        "recallWd = recall_score(predictionsWd, expected_labels_wd, average='macro')\n",
        "cnf_matrixWd = confusion_matrix(predictionsWd, expected_labels_wd)\n",
        "print(cnf_matrixWd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pencEthx-N0-",
        "outputId": "5a6cea52-c645-4db4-80b8-cb4bf4c2037c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.70\n",
            "[[5074   85  306  114  155   49   40   52  539  159]\n",
            " [ 127 5489   14   41   19   17   36   23  238  375]\n",
            " [ 249   48 4142  283  342  224  224  144  105   44]\n",
            " [ 126   41  340 4192  362  887  425  209   99   70]\n",
            " [  68   16  446  214 4370  188  192  194   32   25]\n",
            " [  44   20  259  749  204 4303  115  234   34   25]\n",
            " [  13   18  263  204  206   88 4916   13   22   24]\n",
            " [  66   25  164  152  297  221   17 5100   25   84]\n",
            " [ 111   37   29   28   20    8   18    8 4770   64]\n",
            " [ 122  221   37   23   25   15   17   23  136 5130]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictionsWd, expected_labels_wd, target_names=classNames))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqYQpH2rGdFx",
        "outputId": "25da67d1-0af2-4950-be06-74b4b2d13649"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.85      0.77      0.81      6573\n",
            "  automobile       0.91      0.86      0.89      6379\n",
            "        bird       0.69      0.71      0.70      5805\n",
            "         cat       0.70      0.62      0.66      6751\n",
            "        deer       0.73      0.76      0.74      5745\n",
            "         dog       0.72      0.72      0.72      5987\n",
            "        frog       0.82      0.85      0.84      5767\n",
            "       horse       0.85      0.83      0.84      6151\n",
            "        ship       0.80      0.94      0.86      5093\n",
            "       truck       0.85      0.89      0.87      5749\n",
            "\n",
            "    accuracy                           0.79     60000\n",
            "   macro avg       0.79      0.80      0.79     60000\n",
            "weighted avg       0.79      0.79      0.79     60000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = self.conv1_dropout(out)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = self.conv2_dropout(out)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "D1trMNAQGr9T"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelDropout = NetDropout(n_chans1=32).to(device=device)\n",
        "optimizerDropout = optim.SGD(modelDropout.parameters(), lr=1e-2)\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizerDropout,\n",
        "    model = modelDropout,\n",
        "    loss_fn = lossFn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yicG2GHcGwrk",
        "outputId": "cde1ace3-0469-415c-da6c-4fb33b037325"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 02:58:26.673233 Epoch 1, Training loss 2.0356376956186026\n",
            "2023-12-13 02:58:43.075073 Epoch 2, Training loss 1.777426349842335\n",
            "2023-12-13 02:59:16.257933 Epoch 4, Training loss 1.5672250080596455\n",
            "2023-12-13 02:59:47.929598 Epoch 6, Training loss 1.4729617579513803\n",
            "2023-12-13 03:00:40.138092 Epoch 8, Training loss 1.3992077950626383\n",
            "2023-12-13 03:01:19.349908 Epoch 10, Training loss 1.338284488071871\n",
            "2023-12-13 03:02:12.076162 Epoch 12, Training loss 1.2926866944945987\n",
            "2023-12-13 03:02:53.521293 Epoch 14, Training loss 1.2583493949354763\n",
            "2023-12-13 03:03:30.665549 Epoch 16, Training loss 1.226048232801735\n",
            "2023-12-13 03:04:15.837204 Epoch 18, Training loss 1.2041681144396057\n",
            "2023-12-13 03:05:07.356227 Epoch 20, Training loss 1.181639991376711\n",
            "2023-12-13 03:05:50.726448 Epoch 22, Training loss 1.1565201661318465\n",
            "2023-12-13 03:06:25.111282 Epoch 24, Training loss 1.141395692706413\n",
            "2023-12-13 03:07:00.172785 Epoch 26, Training loss 1.1272767575653009\n",
            "2023-12-13 03:07:33.052962 Epoch 28, Training loss 1.1174984738954803\n",
            "2023-12-13 03:08:04.647211 Epoch 30, Training loss 1.1017405421989959\n",
            "2023-12-13 03:08:37.354782 Epoch 32, Training loss 1.0872736067875572\n",
            "2023-12-13 03:09:10.361815 Epoch 34, Training loss 1.076070168362859\n",
            "2023-12-13 03:09:42.622485 Epoch 36, Training loss 1.068475534086642\n",
            "2023-12-13 03:10:15.299568 Epoch 38, Training loss 1.062724214837984\n",
            "2023-12-13 03:10:47.561935 Epoch 40, Training loss 1.0510337920597448\n",
            "2023-12-13 03:11:19.436390 Epoch 42, Training loss 1.0436432182483966\n",
            "2023-12-13 03:11:52.406495 Epoch 44, Training loss 1.0381479834199256\n",
            "2023-12-13 03:12:24.203776 Epoch 46, Training loss 1.0303069879026974\n",
            "2023-12-13 03:12:55.943920 Epoch 48, Training loss 1.03016659221076\n",
            "2023-12-13 03:13:30.766719 Epoch 50, Training loss 1.0178537367249998\n",
            "2023-12-13 03:14:02.599475 Epoch 52, Training loss 1.014332069917713\n",
            "2023-12-13 03:14:34.793169 Epoch 54, Training loss 1.010195843856353\n",
            "2023-12-13 03:15:15.687616 Epoch 56, Training loss 1.004406998072134\n",
            "2023-12-13 03:15:47.898733 Epoch 58, Training loss 0.9957362796034654\n",
            "2023-12-13 03:16:19.624219 Epoch 60, Training loss 0.9975572569900767\n",
            "2023-12-13 03:16:52.271557 Epoch 62, Training loss 0.9919216998321626\n",
            "2023-12-13 03:17:23.848952 Epoch 64, Training loss 0.9859221282669955\n",
            "2023-12-13 03:17:55.624073 Epoch 66, Training loss 0.9893124439679754\n",
            "2023-12-13 03:18:28.169579 Epoch 68, Training loss 0.9770746781393085\n",
            "2023-12-13 03:18:59.801462 Epoch 70, Training loss 0.9786905018264985\n",
            "2023-12-13 03:19:31.283606 Epoch 72, Training loss 0.9805051396265054\n",
            "2023-12-13 03:20:04.032459 Epoch 74, Training loss 0.9752766580685325\n",
            "2023-12-13 03:20:35.571564 Epoch 76, Training loss 0.9675982501500707\n",
            "2023-12-13 03:21:07.265828 Epoch 78, Training loss 0.9619099104495914\n",
            "2023-12-13 03:21:39.416563 Epoch 80, Training loss 0.9641907974277311\n",
            "2023-12-13 03:22:10.774148 Epoch 82, Training loss 0.9634852165837422\n",
            "2023-12-13 03:22:42.470988 Epoch 84, Training loss 0.9586675811149276\n",
            "2023-12-13 03:23:16.225339 Epoch 86, Training loss 0.9540620915725103\n",
            "2023-12-13 03:23:48.290375 Epoch 88, Training loss 0.958035967767696\n",
            "2023-12-13 03:24:21.010712 Epoch 90, Training loss 0.9540542531043977\n",
            "2023-12-13 03:24:52.590871 Epoch 92, Training loss 0.9479197865861761\n",
            "2023-12-13 03:25:23.975874 Epoch 94, Training loss 0.9473529112003648\n",
            "2023-12-13 03:25:56.987702 Epoch 96, Training loss 0.9505140792073496\n",
            "2023-12-13 03:26:29.037622 Epoch 98, Training loss 0.9451640832149769\n",
            "2023-12-13 03:27:01.485441 Epoch 100, Training loss 0.9408282647504831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracyDropout, predictionsDropout, expected_labels_dropout = validate(modelDropout, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsPKP1SNHckl",
        "outputId": "788a9a01-d664-4fa5-ec0e-1f4abf0d7f7a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.67\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precisionDropout = precision_score(predictionsDropout, expected_labels_dropout, average='macro')\n",
        "recallDropout = recall_score(predictionsDropout, expected_labels_dropout, average='macro')\n",
        "cnf_matrixDropout = confusion_matrix(predictionsDropout, expected_labels_dropout)\n",
        "print(cnf_matrixDropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ4JwwMKHuGW",
        "outputId": "2aa7e6fd-7360-4bdf-c7a7-be227b5074aa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4264  133  415  117  174   53   41   75  558  201]\n",
            " [ 169 4715   63   81   35   44   46   42  296  723]\n",
            " [ 309   48 3139  395  519  386  257  237  122   54]\n",
            " [ 185   98  449 3098  452 1162  504  348  126  130]\n",
            " [ 135   37  576  348 3492  319  290  395   58   51]\n",
            " [  81   32  427  988  290 3305  147  492   49   76]\n",
            " [  78   77  502  553  526  251 4567  117   69   84]\n",
            " [ 106   52  266  242  410  363   53 4143   46  155]\n",
            " [ 414  170  111   81   71   47   51   38 4454  166]\n",
            " [ 259  638   52   97   31   70   44  113  222 4360]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictionsDropout, expected_labels_dropout, target_names=classNames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDBi4vkFIB6H",
        "outputId": "d56821b2-06e0-4bf7-cd63-87c07063531f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.71      0.71      0.71      6031\n",
            "  automobile       0.79      0.76      0.77      6214\n",
            "        bird       0.52      0.57      0.55      5466\n",
            "         cat       0.52      0.47      0.49      6552\n",
            "        deer       0.58      0.61      0.60      5701\n",
            "         dog       0.55      0.56      0.56      5887\n",
            "        frog       0.76      0.67      0.71      6824\n",
            "       horse       0.69      0.71      0.70      5836\n",
            "        ship       0.74      0.79      0.77      5603\n",
            "       truck       0.73      0.74      0.73      5886\n",
            "\n",
            "    accuracy                           0.66     60000\n",
            "   macro avg       0.66      0.66      0.66     60000\n",
            "weighted avg       0.66      0.66      0.66     60000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NetBatchNorm(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1_batchnorm(self.conv1(x))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = self.conv2_batchnorm(self.conv2(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "KP93C8igIKLR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_batch_norm = NetBatchNorm(n_chans1=32).to(device=device)\n",
        "optimizer_batch_norm = optim.SGD(model_batch_norm.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer_batch_norm,\n",
        "    model = model_batch_norm,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAGqjqU_INsm",
        "outputId": "7c1c5b4a-2eb7-4e0e-fb7a-56b1213b62af"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 03:30:01.004776 Epoch 1, Training loss 1.7788722178210383\n",
            "2023-12-13 03:30:17.505987 Epoch 2, Training loss 1.4811666915788675\n",
            "2023-12-13 03:30:51.403177 Epoch 4, Training loss 1.257535622400396\n",
            "2023-12-13 03:31:24.814614 Epoch 6, Training loss 1.1194834946214085\n",
            "2023-12-13 03:31:59.029620 Epoch 8, Training loss 1.0272434318004666\n",
            "2023-12-13 03:32:32.247301 Epoch 10, Training loss 0.9607238463123741\n",
            "2023-12-13 03:33:06.013086 Epoch 12, Training loss 0.9091478647935726\n",
            "2023-12-13 03:33:39.238659 Epoch 14, Training loss 0.8679635697008704\n",
            "2023-12-13 03:34:13.152799 Epoch 16, Training loss 0.8341454881079057\n",
            "2023-12-13 03:34:47.004540 Epoch 18, Training loss 0.8052948688911965\n",
            "2023-12-13 03:35:20.588938 Epoch 20, Training loss 0.7799882767221812\n",
            "2023-12-13 03:35:54.304709 Epoch 22, Training loss 0.7573165939287152\n",
            "2023-12-13 03:36:27.159817 Epoch 24, Training loss 0.7367510805883066\n",
            "2023-12-13 03:37:00.751721 Epoch 26, Training loss 0.7178838554474399\n",
            "2023-12-13 03:37:33.060604 Epoch 28, Training loss 0.7003214621864011\n",
            "2023-12-13 03:38:06.347350 Epoch 30, Training loss 0.6837001131166278\n",
            "2023-12-13 03:38:38.680134 Epoch 32, Training loss 0.668095327208719\n",
            "2023-12-13 03:39:11.150505 Epoch 34, Training loss 0.6532269172046495\n",
            "2023-12-13 03:39:44.313391 Epoch 36, Training loss 0.6391663206050463\n",
            "2023-12-13 03:40:16.680399 Epoch 38, Training loss 0.6257045730910338\n",
            "2023-12-13 03:40:50.294894 Epoch 40, Training loss 0.612739170718071\n",
            "2023-12-13 03:41:22.528048 Epoch 42, Training loss 0.6002363123933373\n",
            "2023-12-13 03:41:55.106439 Epoch 44, Training loss 0.5880871586634985\n",
            "2023-12-13 03:42:28.016669 Epoch 46, Training loss 0.5763296151100217\n",
            "2023-12-13 03:43:00.328948 Epoch 48, Training loss 0.564958896306927\n",
            "2023-12-13 03:43:33.776733 Epoch 50, Training loss 0.5540009592957509\n",
            "2023-12-13 03:44:06.299091 Epoch 52, Training loss 0.5433196700594919\n",
            "2023-12-13 03:44:39.544540 Epoch 54, Training loss 0.5328096078179986\n",
            "2023-12-13 03:45:12.030558 Epoch 56, Training loss 0.5225355470805522\n",
            "2023-12-13 03:45:44.419655 Epoch 58, Training loss 0.5123967955560635\n",
            "2023-12-13 03:46:17.785623 Epoch 60, Training loss 0.5025949601817619\n",
            "2023-12-13 03:46:49.968368 Epoch 62, Training loss 0.49293298889761383\n",
            "2023-12-13 03:47:23.131571 Epoch 64, Training loss 0.48345923318963524\n",
            "2023-12-13 03:47:55.512619 Epoch 66, Training loss 0.47426989543087344\n",
            "2023-12-13 03:48:28.140676 Epoch 68, Training loss 0.46530851775118154\n",
            "2023-12-13 03:49:01.492699 Epoch 70, Training loss 0.4565478113415601\n",
            "2023-12-13 03:49:33.813550 Epoch 72, Training loss 0.44809642054921833\n",
            "2023-12-13 03:50:07.428948 Epoch 74, Training loss 0.43980702117580894\n",
            "2023-12-13 03:50:39.931226 Epoch 76, Training loss 0.43164773805595724\n",
            "2023-12-13 03:51:13.272748 Epoch 78, Training loss 0.42371094261136505\n",
            "2023-12-13 03:51:45.867507 Epoch 80, Training loss 0.4159796793594995\n",
            "2023-12-13 03:52:18.662843 Epoch 82, Training loss 0.4081952147700293\n",
            "2023-12-13 03:52:53.677167 Epoch 84, Training loss 0.40053083082599106\n",
            "2023-12-13 03:53:26.589361 Epoch 86, Training loss 0.39299272446681166\n",
            "2023-12-13 03:54:00.148546 Epoch 88, Training loss 0.38550486630948305\n",
            "2023-12-13 03:54:32.906294 Epoch 90, Training loss 0.37818269711702374\n",
            "2023-12-13 03:55:06.509241 Epoch 92, Training loss 0.3710045700373552\n",
            "2023-12-13 03:55:42.139773 Epoch 94, Training loss 0.3638934810048022\n",
            "2023-12-13 03:56:16.124297 Epoch 96, Training loss 0.35684318288855843\n",
            "2023-12-13 03:56:50.359158 Epoch 98, Training loss 0.34995578354238854\n",
            "2023-12-13 03:57:24.447172 Epoch 100, Training loss 0.3431912362956635\n",
            "2023-12-13 03:57:59.179691 Epoch 102, Training loss 0.33665633337843753\n",
            "2023-12-13 03:58:33.706920 Epoch 104, Training loss 0.3301611102431479\n",
            "2023-12-13 03:59:09.091073 Epoch 106, Training loss 0.32387375480035685\n",
            "2023-12-13 03:59:43.453983 Epoch 108, Training loss 0.31768571234801235\n",
            "2023-12-13 04:00:18.272850 Epoch 110, Training loss 0.3114882885571331\n",
            "2023-12-13 04:00:51.713911 Epoch 112, Training loss 0.30553584570622505\n",
            "2023-12-13 04:01:27.319393 Epoch 114, Training loss 0.29964252158313454\n",
            "2023-12-13 04:02:00.894356 Epoch 116, Training loss 0.2939393352193143\n",
            "2023-12-13 04:02:34.432725 Epoch 118, Training loss 0.28828214798741936\n",
            "2023-12-13 04:03:06.959399 Epoch 120, Training loss 0.2827920187502871\n",
            "2023-12-13 04:03:40.332890 Epoch 122, Training loss 0.2773753420623672\n",
            "2023-12-13 04:04:12.802744 Epoch 124, Training loss 0.2720298362262261\n",
            "2023-12-13 04:04:45.535201 Epoch 126, Training loss 0.2668689063719243\n",
            "2023-12-13 04:05:18.946894 Epoch 128, Training loss 0.26175895346628736\n",
            "2023-12-13 04:05:51.528317 Epoch 130, Training loss 0.25670120917984746\n",
            "2023-12-13 04:06:24.782118 Epoch 132, Training loss 0.25181413164643374\n",
            "2023-12-13 04:06:57.033857 Epoch 134, Training loss 0.24698886780730447\n",
            "2023-12-13 04:07:29.881128 Epoch 136, Training loss 0.24223714889220113\n",
            "2023-12-13 04:08:02.455516 Epoch 138, Training loss 0.23767376831158651\n",
            "2023-12-13 04:08:34.514970 Epoch 140, Training loss 0.233078879049367\n",
            "2023-12-13 04:09:07.778673 Epoch 142, Training loss 0.22860896829372782\n",
            "2023-12-13 04:09:40.021811 Epoch 144, Training loss 0.22419293599722484\n",
            "2023-12-13 04:10:13.436356 Epoch 146, Training loss 0.2199140620772796\n",
            "2023-12-13 04:10:46.134990 Epoch 148, Training loss 0.21570491896527808\n",
            "2023-12-13 04:11:18.990401 Epoch 150, Training loss 0.2116584700658498\n",
            "2023-12-13 04:11:52.017123 Epoch 152, Training loss 0.20770319072467744\n",
            "2023-12-13 04:12:24.467973 Epoch 154, Training loss 0.20390992928439242\n",
            "2023-12-13 04:12:57.612502 Epoch 156, Training loss 0.20016891275868393\n",
            "2023-12-13 04:13:29.944507 Epoch 158, Training loss 0.19638979397333034\n",
            "2023-12-13 04:14:02.706342 Epoch 160, Training loss 0.19282187995931988\n",
            "2023-12-13 04:14:35.534313 Epoch 162, Training loss 0.1893287526534113\n",
            "2023-12-13 04:15:07.803253 Epoch 164, Training loss 0.18590120769217802\n",
            "2023-12-13 04:15:41.027396 Epoch 166, Training loss 0.18256066867705348\n",
            "2023-12-13 04:16:13.559337 Epoch 168, Training loss 0.1793203869515368\n",
            "2023-12-13 04:16:46.580309 Epoch 170, Training loss 0.17609058871693775\n",
            "2023-12-13 04:17:19.360367 Epoch 172, Training loss 0.173097914442077\n",
            "2023-12-13 04:17:51.672282 Epoch 174, Training loss 0.17000619670295197\n",
            "2023-12-13 04:18:24.974756 Epoch 176, Training loss 0.166992456511692\n",
            "2023-12-13 04:18:57.231056 Epoch 178, Training loss 0.16416927952500407\n",
            "2023-12-13 04:19:30.474013 Epoch 180, Training loss 0.16133454108558348\n",
            "2023-12-13 04:20:02.866368 Epoch 182, Training loss 0.15855552982110196\n",
            "2023-12-13 04:20:35.523609 Epoch 184, Training loss 0.15586245392718354\n",
            "2023-12-13 04:21:08.572605 Epoch 186, Training loss 0.15317165745836694\n",
            "2023-12-13 04:21:40.854722 Epoch 188, Training loss 0.15051819568457048\n",
            "2023-12-13 04:22:15.430427 Epoch 190, Training loss 0.14791455196069025\n",
            "2023-12-13 04:22:47.701841 Epoch 192, Training loss 0.14541652666457244\n",
            "2023-12-13 04:23:20.539297 Epoch 194, Training loss 0.14288485509431575\n",
            "2023-12-13 04:23:52.848189 Epoch 196, Training loss 0.14047358956550013\n",
            "2023-12-13 04:24:25.350217 Epoch 198, Training loss 0.13820820816023194\n",
            "2023-12-13 04:24:58.655747 Epoch 200, Training loss 0.13588069860473315\n",
            "2023-12-13 04:25:31.008060 Epoch 202, Training loss 0.13357520347122875\n",
            "2023-12-13 04:26:04.386651 Epoch 204, Training loss 0.13141780212292892\n",
            "2023-12-13 04:26:36.935645 Epoch 206, Training loss 0.1293126845784733\n",
            "2023-12-13 04:27:09.875610 Epoch 208, Training loss 0.1276917410085497\n",
            "2023-12-13 04:27:42.993624 Epoch 210, Training loss 0.12505717702267116\n",
            "2023-12-13 04:28:15.438796 Epoch 212, Training loss 0.12321704490315122\n",
            "2023-12-13 04:28:48.759634 Epoch 214, Training loss 0.12220905001854043\n",
            "2023-12-13 04:29:21.194852 Epoch 216, Training loss 0.11930388543764343\n",
            "2023-12-13 04:29:54.435248 Epoch 218, Training loss 0.118358223894349\n",
            "2023-12-13 04:30:27.138527 Epoch 220, Training loss 0.11598461969991399\n",
            "2023-12-13 04:30:59.508678 Epoch 222, Training loss 0.1139767643688318\n",
            "2023-12-13 04:31:33.144690 Epoch 224, Training loss 0.11340329056496129\n",
            "2023-12-13 04:32:05.305842 Epoch 226, Training loss 0.11017129967546524\n",
            "2023-12-13 04:32:38.755805 Epoch 228, Training loss 0.11026593373940728\n",
            "2023-12-13 04:33:10.931248 Epoch 230, Training loss 0.10668299751132346\n",
            "2023-12-13 04:33:43.656996 Epoch 232, Training loss 0.10493010188193272\n",
            "2023-12-13 04:34:16.610527 Epoch 234, Training loss 0.10327880885309118\n",
            "2023-12-13 04:34:49.172368 Epoch 236, Training loss 0.10160945180108023\n",
            "2023-12-13 04:35:22.817060 Epoch 238, Training loss 0.1002612708474669\n",
            "2023-12-13 04:35:55.388944 Epoch 240, Training loss 0.09915543450733356\n",
            "2023-12-13 04:36:28.770980 Epoch 242, Training loss 0.09698443805508296\n",
            "2023-12-13 04:37:01.308259 Epoch 244, Training loss 0.09555165232885676\n",
            "2023-12-13 04:37:33.899062 Epoch 246, Training loss 0.0941551606173215\n",
            "2023-12-13 04:38:07.446111 Epoch 248, Training loss 0.09319256595752733\n",
            "2023-12-13 04:38:40.138760 Epoch 250, Training loss 0.09168486097527434\n",
            "2023-12-13 04:39:14.916997 Epoch 252, Training loss 0.09005545352673744\n",
            "2023-12-13 04:39:47.562766 Epoch 254, Training loss 0.08871621928175392\n",
            "2023-12-13 04:40:21.036822 Epoch 256, Training loss 0.08741833903538564\n",
            "2023-12-13 04:40:53.982677 Epoch 258, Training loss 0.08614655730702803\n",
            "2023-12-13 04:41:26.881206 Epoch 260, Training loss 0.08487325662013401\n",
            "2023-12-13 04:42:00.444843 Epoch 262, Training loss 0.0836602175451072\n",
            "2023-12-13 04:42:32.755015 Epoch 264, Training loss 0.08246736009569501\n",
            "2023-12-13 04:43:06.185991 Epoch 266, Training loss 0.08131242084109684\n",
            "2023-12-13 04:43:38.911814 Epoch 268, Training loss 0.08014837845855052\n",
            "2023-12-13 04:44:12.155179 Epoch 270, Training loss 0.0790310256216494\n",
            "2023-12-13 04:44:44.837400 Epoch 272, Training loss 0.07791296340753813\n",
            "2023-12-13 04:45:17.359164 Epoch 274, Training loss 0.0768785627970896\n",
            "2023-12-13 04:45:50.883366 Epoch 276, Training loss 0.07582658874180616\n",
            "2023-12-13 04:46:23.347833 Epoch 278, Training loss 0.07484136810859718\n",
            "2023-12-13 04:46:56.950683 Epoch 280, Training loss 0.07383417222848938\n",
            "2023-12-13 04:47:29.453970 Epoch 282, Training loss 0.07284905513167343\n",
            "2023-12-13 04:48:02.456180 Epoch 284, Training loss 0.0719008623596157\n",
            "2023-12-13 04:48:35.808645 Epoch 286, Training loss 0.07093289863947026\n",
            "2023-12-13 04:49:08.417520 Epoch 288, Training loss 0.06999161907607485\n",
            "2023-12-13 04:49:41.994206 Epoch 290, Training loss 0.06909566836388749\n",
            "2023-12-13 04:50:14.721158 Epoch 292, Training loss 0.06820796734041265\n",
            "2023-12-13 04:50:48.412938 Epoch 294, Training loss 0.06732219069019493\n",
            "2023-12-13 04:51:21.013805 Epoch 296, Training loss 0.0664780087330762\n",
            "2023-12-13 04:51:54.242254 Epoch 298, Training loss 0.06562982037391923\n",
            "2023-12-13 04:52:27.257568 Epoch 300, Training loss 0.06480508374378961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_batchNorm, predictions_batchNorm, expected_labels_batchNorm = validate(modelDropout, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHuwJBjMITSF",
        "outputId": "79ed9a95-33b2-45be-e588-0de946968923"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.67\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_batchNorm = precision_score(predictions_batchNorm, expected_labels_batchNorm, average='macro')\n",
        "recall_batchNorm = recall_score(predictions_batchNorm, expected_labels_batchNorm, average='macro')\n",
        "cnf_matrix_batchNorm = confusion_matrix(predictions_batchNorm, expected_labels_batchNorm)\n",
        "print(cnf_matrix_batchNorm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdHD8UHRIdGP",
        "outputId": "4f2f2fd3-613e-4447-d5f4-76fc6f3bb290"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4257  127  420  125  183   58   42   60  562  207]\n",
            " [ 178 4709   36   80   28   49   60   41  307  692]\n",
            " [ 318   48 3179  397  575  376  270  231  114   47]\n",
            " [ 179   74  474 3131  454 1158  445  350  133  160]\n",
            " [ 114   43  565  326 3488  315  281  439   71   48]\n",
            " [  78   33  426  977  244 3327  151  467   48   65]\n",
            " [  65   91  475  550  524  276 4588  123   62   78]\n",
            " [ 105   40  255  220  397  347   68 4127   47  182]\n",
            " [ 426  183  118   76   66   45   50   39 4416  151]\n",
            " [ 280  652   52  118   41   49   45  123  240 4370]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictionsDropout, expected_labels_dropout, target_names=classNames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CzUP91zIuAH",
        "outputId": "491ca0bc-3e30-4e79-9f95-cb9ff7dd67da"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.71      0.71      0.71      6031\n",
            "  automobile       0.79      0.76      0.77      6214\n",
            "        bird       0.52      0.57      0.55      5466\n",
            "         cat       0.52      0.47      0.49      6552\n",
            "        deer       0.58      0.61      0.60      5701\n",
            "         dog       0.55      0.56      0.56      5887\n",
            "        frog       0.76      0.67      0.71      6824\n",
            "       horse       0.69      0.71      0.70      5836\n",
            "        ship       0.74      0.79      0.77      5603\n",
            "       truck       0.73      0.74      0.73      5886\n",
            "\n",
            "    accuracy                           0.66     60000\n",
            "   macro avg       0.66      0.66      0.66     60000\n",
            "weighted avg       0.66      0.66      0.66     60000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}